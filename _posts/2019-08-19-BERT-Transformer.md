---
layout: post
title:  "预训练语言模型BERT学习之四 - Transformer"
date:   2019-08-19 09:40:45 +0800
categories: [AI, NLP]
tags: 
 - AI
 - NLP
 - NER
 - Transformer
 - Attention
---





## 更多参考资源:

[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)    
[Depthwise Separable Convolutions for Neural Machine Translation](https://arxiv.org/pdf/1706.03059.pdf)    
[Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/pdf/1801.10198.pdf)    
[Self-Attention with Relative Position Representations](https://arxiv.org/pdf/1803.02155.pdf)    
[Training Tips for the Transformer Model](https://arxiv.org/pdf/1804.00247.pdf)    
[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)










