---
layout: post
title:  "预训练语言模型BERT学习之二"
date:   2019-08-16 15:05:45 +0800
categories: [AI, NLP]
tags:
 - AI
 - NLP
 - NER
 - Transformer
 - Attention
---


下面介绍Google发表的BERT[论文]((https://arxiv.org/pdf/1810.04805.pdf))。

### 回顾
现在有很多利用预训练的语言表征来完成下游NLP任务的研究，作者把它们概括为两类feature-based和fine-tuning:

分类     | 代表 | task-specific模型 | 使用方案
---------|-----|-------------------|---------
feature-based | ELMo | 需要 | 把表征作为feature提供给下游任务
fine-tuning	 | OpenAI GPT， ULMFiT | 不需要	 | fine tune预训练的参数

这两类方法的共性在于它们在预训练中都使用了一样的目标函数，也都使用了单向的语言模型。
作者对这些方法的批评在于它们没有很好的利用上下文的信息。尽管如ELMo这样的算法利用了正向和反向的语言模型，可本质上仍然是两个unidirectional模型的叠加。对于SQuAD这种阅读理解式的任务，能够同时从两个方向提取context信息至关重要，然而现存的方法有巨大的局限性。

BERT, OpenAI GPT, 和ELMo之间的区别如图示：
![](/assets/images/BERT_Intro2/02.png)

### 创新
作为fine-tuning这一类的方法，作者提出了改进的方案：BERT(Bidirectional Encoder Representations from Transformers)
具体做法是，

1. 采取新的预训练的目标函数：the “masked language model” (MLM) 随机mask输入中的一些tokens，然后在预训练中对它们进行预测。这样做的好处是学习到的表征能够融合两个方向上的context。这个做法我觉得非常像skip-gram。过去的同类算法在这里有所欠缺，比如上文提到的ELMo，它用的是两个单向的LSTM然后把结果拼接起来；还有OpenAI GPT，虽然它一样使用了transformer，但是只利用了一个方向的注意力机制，本质上也一样是单项的语言模型。
2. 增加句子级别的任务：“next sentence prediction”
作者认为很多NLP任务比如QA和NLI都需要对两个句子之间关系的理解，而语言模型不能很好的直接产生这种理解。为了理解句子关系，作者同时pre-train了一个“next sentence prediction”任务。具体做法是随机替换一些句子，然后利用上一句进行IsNext/NotNext的预测。
在实际的预训练中，这两个任务是jointly training

### BERT模型
#### 模型架构
论文使用了两种模型：
* BERT Base  : L=12, H=768, A=12, Total Parameters=110M
* BERT Large : L=24, H=1024, A=16, Total Parameters=340M
这里L是layers层数(即Transformer blocks个数)，H是hidden vector size, A是self-attention的“头数”。
在NLP领域，10层以上的layer还是比较“惊人”的，印象中当年Attention is All You Need第一次提出transformer的时候，在MT任务中用到了6层。当然从结构上来讲，transformers之间用的是residual connection，并且有batch normarlization这种“常规”操作，多层不是什么问题。有意思的是在于这么多层的结构究竟学到了什么？NLP不能和CV做简单的类比，网络层数并不是“多多益善”；有论点认为低层偏向于语法特征学习，高层偏向于语义特征学习。

#### 输入的表示
针对不同的任务，模型能够明确的表达一个句子，或者句子对(比如[问题，答案])。对于每一个token, 它的表征由其对应的token embedding, 段表征(segment embedding)和位置表征(position embedding)相加产生。如下图：
![](/assets/images/BERT_Intro2/03.png)

具体细节如下：

论文使用了[WordPiece embeddings](https://arxiv.org/pdf/1609.08144.pdf)
使用了positional embeddings， 长达512位，即句子的最大长度
每句话的第一个token总是[CLS]。对应它的最终的hidden state(即Transformer的输出)用来表征整个句子，可以用于下游的分类任务。
模型能够处理句子对。为区别两个句子，用一个特殊token [SEP]隔开它们，另外针对不同的句子，把学习到的Segment embeddings 加到每个token的embedding上(如图)
对于单个句子仅使用一个Segment embedding

#### 预训练的任务
*Masked LM*

具体细节论文上解释的很清楚了，这里从略了。
提一下值得注意的一点吧：为了达到真正的bidirectional的LM的效果，作者创新性的提出了Masked LM，但是缺点是如果常常把一些词mask起来，未来的fine tuning过程中模型有可能没见过这些词。这个量积累下来还是很大的。因为作者在他的实现中随机选择了句子中15%的WordPiece tokens作为要mask的词。
为了解决这个问题，作者在做mask的时候，

80%的时间真的用[MASK]取代被选中的词。比如 my dog is hairy -> my dog is [MASK]
10%的时间用一个随机词取代它：my dog is hairy -> my dog is apple
10%的时间保持不变: my dog is hairy -> my dog is hairy
为什么要以一定的概率保持不变呢？ 这是因为刚才说了，如果100%的时间都用[MASK]来取代被选中的词，那么在fine tuning的时候模型会有一些没见过的词。那么为啥要以一定的概率使用随机词呢？这是因为Transformer要保持对每个输入token分布式的表征，否则Transformer很可能会记住这个[MASK]就是"hairy"。至于使用随机词带来的负面影响，文章中说了,所有其他的token(即非"hairy"的token)共享15%*10% = 1.5%的概率，其影响是可以忽略不计的。

*Next Sentence Prediction*

具体做法见论文，这里仍然从略。
简单说一下这么做的原因。很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，而语言模型并不能直接反应这种关系。为了是预训练出来的模型很好的适应这些任务，作者提出了这样的一个预训练任务。实验表明，增加这样的一个任务在针对下游的QA和NLI任务时效果非常好。

在预训练中文章使用了BooksCorpus(800M 词)和English Wikipedia(2,500M 词)。

#### 微调过程
对于句子级的分类任务，BERT的微调方法非常直观。论文用刚才介绍过的特殊符号[CLS]来对应整个句子的表征。我们只需要把它作为输入通过一层网络，最后做softmax就可以了。

### 实验
*GLUE*
[GLUE](https://gluebenchmark.com/)是一个自然语言任务集合，它包括以下这些数据集:

名称 |	全名 |	用途
-----|------|---------
MNLI |	Multi-Genre NLI	| 蕴含关系推断
QQP |	Quora Question Pairs |	问题对是否等价
QNLI |	Question NLI |	句子是否回答问句
SST-2 |	Stanford Sentiment Treebank	 | 情感分析
CoLA |	Corpus of Linguistic Acceptability |	句子语言性判断
STS-B |	Semantic Textual Similarity	| 语义相似
MRPC |	Microsoft Research Paraphrase Corpus |	句子对是否语义等价
RTE	 | Recognizing Texual Entailment |	蕴含关系推断
WNLI |	Winograd NLI	| 蕴含关系推断
包含了各种理解句子含义和关系的任务。看一下效果：参见[论文 Table 1](https://arxiv.org/pdf/1810.04805.pdf)。

BERT在每一个单项上的表现都是最优。一个很有意思的现象是：在所有的任务上BERT Large远超过BERT Base，其中甚至包括那些仅有少量训练数据的任务。

*Ablation Studies*
BERT本身包含了很多创新点，下面看一下各个部分的贡献。
先看一下pre-training中所用到的各种技术的重要性。参见[论文 Table 5](https://arxiv.org/pdf/1810.04805.pdf)。

* No NSP ： 没有next sentence prediction(NSP)任务
* LTR & No NSP: 没有MLM，而是用Left-to-Right(LTR) LM
* +BiLSTM： fine-tuning过程中，在LTR & No NSP上增加一个随机初始化的BiLSTM

另外作者还实验了模型大小，预训练步数，已经feature-based版本的BERT，实验结果也非常令人信服。特别要强调的一点是：我们知道在大型任务中大的模型会带来持续的效果增长，但是这是我们第一次看见，只要模型经过了足够的预训练，在小任务中大模型也能够带来显著的增长。

### 总结
BERT是一项里程碑式的工作。这是利用语言模型预训练然后再fine-tuning这条路的成功。技术上来讲，BERT使用了masked lm和下一句预测的联合训练方法，在加上12层BERT Base的transformer，超过ELMo,ULMFiT,OpenAI GPT; 从资源上讲，计算资源开销巨大，个人很难复现其工作。很多人会利用Google发布模型。它有可能是CV领域利用image net预训练出的那些著名模型在NLP领域的对等物。这也是Transformer的巨大胜利，它的self attention机制和position embedding相信会赢得更大的关注。

### 新闻
#### Nvidia训练出世界最大语言模型MegatronLM，模型使用了83亿个参数，比BERT大24倍，比OpenAI的GPT-2大5倍。
AI硬件厂商英伟达今天宣布，他们在NLP模型上取得了三大突破，为今后会话AI的落地应用铺平了道路。英伟达用自己的硬件与并行计算软件相结合，在BERT模型的训练和推理上创下三项世界纪录：
* 最快的BERT训练速度，只需53分钟
* 最快的BERT推理速度，只需2.2ms
* 最大的BERT模型，包含83亿参数

今年以来，NLP领域的“军备竞赛”越来越激烈。基于Transformer架构的模型，如谷歌的[XLNet](/assets/images/BERT_Intro2/1906.08237.pdf)和Facebook的RoBERTa，在GLUE、SQuAD这些基准测试榜单头名争夺中你方唱罢我登场。两大公司为了刷榜的消耗了大量的时间和计算资源。为了提高BERT的训练速度，谷歌堆上了1024块TPU，用76分钟训练出了BERT模型。Facebook用上了1024个英伟达V100 GPU。英伟达也不甘示弱，用1472个V100 GPU首次实现在一个小时之内训练完BERT模型。英伟达表示，这项研究能够帮助企业使用实时会话AI更自然地与客户互动，帮助开发人员将最先进的NLP模型大规模部署在应用程序中。

##### 最大的BERT模型
英伟达使用了92个DGX-2H节点、1,472个V100 GPU的DGX SuperPOD系统来训练BERT模型，将BERT-Large训练时间从几天缩短到到53分钟。
英伟达也给出更低配置下的训练速度。他们在“只有”16个V100 GPU的NVIDIA DGX-2用2.8天内训练了BERT-Large，展示了英伟达GPU对于会话AI的可扩展性。考虑到开发人员对大型NLP模型不断增长的需求，英伟达建立并训练了世界上最大的基于Transformer的语言模型GPT-2 8B，它具有83亿个参数，是BERT-Large的24倍、GPT-2的5.6倍。
![](/assets/images/BERT_Intro2/01.png)
GPT-2 8B模型使用原生PyTorch进行训练，在512 GPU上有8路模型并行和的64路数据并行。实验是在DGX SuperPOD上进行的，其基准模型为12亿个参数，适用于单个V100 GPU。在单个GPU上运行此基线模型的端到端训练pipeline可实现39 TeraFLOPS的算力，这是V100 GPU的理论峰值算力的30％。通过8路并行模型将模型扩展到512个GPU上的83亿个参数，NVIDIA团队在整个应用程序中实现了高达15.1 PetaFLOPS的持续性能，并且与基线相比达到了76％的扩展效率。
![DGX SuperPOD](/assets/images/BERT_Intro2/nvidia-dgx-superpod-820x461.jpg)

##### 最快的推理速度
上面的推理部分太过“烧钱”，而英伟达在BERT推理上取得的进步可以让普通开发者也能用上。因为现在一些实时处理的任务也开始用上了Transformer，比如语音合成（TTS）、语音识别（ASR）。
但是Transformer在这些落地化场景中却遇到了困难。这是因为在推理部署阶段，计算性能会面临巨大的挑战。以BERT为原型的多层Transformer模型，其性能常常难以满足在线业务对于低延迟和高吞吐的要求。于是，英伟达提出了Faster Transformer，并在GitHub上开源了代码。Faster Transformer是一个BERT Transformer单层前向计算的高效实现，代码简洁，后续可以通过简单修改支持多种Transformer结构。目前优化集中在编码器（encoder）的前向计算。底层由CUDA和cuBLAS实现，支持FP16和FP32两种计算模式，其中FP16可以充分利用Volta和Turing架构GPU上的张量核心单元。英伟达使用运行TensorRT的T4 GPU，仅在2.2毫秒内就对BERT-Base SQuAD数据集进行了推理，远低于许多实时应用的10毫秒处理阈值。英伟达测试了不同生产环境下Faster Transformer前向计算的执行时间以及与TensorFlow XLA的性能比较。固定句子长度为32，标准模型和裁剪模型在不同batch size下，12层Transformer在V100上实现了2倍的速度提升。

## 更多参考资源:

[Google论文 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)    
[BERT – State of the Art Language Model for NLP](https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/)    
[【NLP】Attention原理和源码解析](https://zhuanlan.zhihu.com/p/43493999)    
[【NLP】Transformer详解](https://zhuanlan.zhihu.com/p/44121378)    
[【NLP】语言模型和迁移学习](https://zhuanlan.zhihu.com/p/42618178)    
[GitHub上的BERT的代码](https://github.com/google-research/bert)    
[Nvidia训练出83亿参数世界最大语言模型MegatronLM，53分钟成功训练BERT](https://nvidianews.nvidia.com/news/nvidia-achieves-breakthroughs-in-language-understandingto-enable-real-time-conversational-ai)    
[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)    
[Transformer开源地址](https://github.com/NVIDIA/Megatron-LMFaster)    
[Transformer开源地址](https://github.com/NVIDIA/DeepLearningExamples/tree/master/FasterTransformer)    










